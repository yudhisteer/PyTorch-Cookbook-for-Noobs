{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so before we dive into PyTorch let's see another framework called NumPy. We can do a lot of the same things with NumPy that we can do with PyTorch. However, Numpy does not concerned with how to keep track of gradients, computational graphs, or automatic differentiation. So in summary, Numpy is not a deep learning framework, it is a general purpose numerical computing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # NumPy for numerical operations and array handling\n",
    "from rich import print  # Rich library for enhanced console output formatting  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network dimensions\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "# N: batch size (number of samples processed together)\n",
    "# D_in: input dimension (number of input features)\n",
    "# H: hidden layer dimension (number of neurons in hidden layer)\n",
    "# D_out: output dimension (number of output classes/values)\n",
    "\n",
    "print(\"N: \", N)\n",
    "print(\"D_in: \", D_in)\n",
    "print(\"H: \", H)\n",
    "print(\"D_out: \", D_out)\n",
    "\n",
    "# Create random input and output data for training\n",
    "# x: input data matrix of shape (N, D_in) - each row is a sample, each column is a feature\n",
    "x = np.random.randn(N, D_in)\n",
    "# y: target/ground truth data matrix of shape (N, D_out) - what we want the network to output\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights using normal distribution\n",
    "# w1: weights connecting input layer to hidden layer, shape (D_in, H)\n",
    "w1 = np.random.randn(D_in, H)\n",
    "# w2: weights connecting hidden layer to output layer, shape (H, D_out)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "# print(\"x: \", x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "# print(\"y: \", y)\n",
    "print(\"y.shape: \", y.shape)\n",
    "# print(\"w1: \", w1)\n",
    "print(\"w1.shape: \", w1.shape)\n",
    "# print(\"w2: \", w2)\n",
    "print(\"w2.shape: \", w2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 1e-6  # Step size for weight updates (how fast the model learns)\n",
    "                      # Small value (1e-6) ensures stable but slow learning\n",
    "iterations = 500      # Number of training epochs (complete passes through the data)\n",
    "\n",
    "print(\"learning_rate: \", learning_rate)\n",
    "print(\"iterations: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop - implement gradient descent manually\n",
    "for i in range(iterations):\n",
    "    \n",
    "    # FORWARD PASS: compute predicted output\n",
    "    # Step 1: Linear transformation from input to hidden layer\n",
    "    h = x.dot(w1)  # Matrix multiplication: (N, D_in) × (D_in, H) = (N, H)\n",
    "    \n",
    "    # Step 2: Apply ReLU activation function to hidden layer\n",
    "    # ReLU(x) = max(0, x) - sets negative values to 0, keeps positive values unchanged\n",
    "    h_relu = np.maximum(h, 0)  # Element-wise maximum with 0\n",
    "    \n",
    "    # Step 3: Linear transformation from hidden layer to output\n",
    "    y_pred = h_relu.dot(w2)  # Matrix multiplication: (N, H) × (H, D_out) = (N, D_out)\n",
    "\n",
    "    # LOSS COMPUTATION: Mean Squared Error (MSE)\n",
    "    # Calculate the difference between predicted and actual values, square it, and sum\n",
    "    loss = np.square(y_pred - y).sum()  # Sum of squared errors across all samples and outputs\n",
    "    print(f\"Iteration {i}: loss = {loss}\")\n",
    "\n",
    "    # BACKWARD PASS: compute gradients using chain rule of calculus\n",
    "    # We need to find how much each weight contributes to the loss (partial derivatives)\n",
    "    \n",
    "    # Step 1: Gradient of loss with respect to predictions\n",
    "    # d(loss)/d(y_pred) = 2 * (y_pred - y) [derivative of squared error]\n",
    "    grad_y_pred = 2.0 * (y_pred - y)  # Shape: (N, D_out)\n",
    "    \n",
    "    # Step 2: Gradient of loss with respect to w2\n",
    "    # Using chain rule: d(loss)/d(w2) = d(loss)/d(y_pred) × d(y_pred)/d(w2)\n",
    "    # Since y_pred = h_relu.dot(w2), d(y_pred)/d(w2) = h_relu.T\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)  # Shape: (H, D_out)\n",
    "    \n",
    "    # Step 3: Gradient of loss with respect to hidden layer (after ReLU)\n",
    "    # d(loss)/d(h_relu) = d(loss)/d(y_pred) × d(y_pred)/d(h_relu) = grad_y_pred × w2.T\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)  # Shape: (N, H)\n",
    "    \n",
    "    # Step 4: Gradient of loss with respect to hidden layer (before ReLU)\n",
    "    # ReLU derivative: 1 if input > 0, 0 if input <= 0\n",
    "    grad_h = grad_h_relu.copy()  # Start with gradient after ReLU\n",
    "    grad_h[h < 0] = 0  # Set gradient to 0 where original h was negative (ReLU derivative)\n",
    "    \n",
    "    # Step 5: Gradient of loss with respect to w1\n",
    "    # d(loss)/d(w1) = d(loss)/d(h) × d(h)/d(w1) = grad_h × x.T\n",
    "    grad_w1 = x.T.dot(grad_h)  # Shape: (D_in, H)\n",
    "    \n",
    "    # WEIGHT UPDATE: Apply gradient descent\n",
    "    # New weight = Old weight - learning_rate × gradient\n",
    "    # This moves weights in the direction that reduces the loss\n",
    "    w1 -= learning_rate * grad_w1  # Update input-to-hidden weights\n",
    "    w2 -= learning_rate * grad_w2  # Update hidden-to-output weights "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb2fb3c",
   "metadata": {},
   "source": [
    "# What is a tensor?\n",
    "\n",
    "Based on: https://docs.pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92cde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fceadf8",
   "metadata": {},
   "source": [
    "### Initilizing  Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffc5ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape of tensor x_data:  torch.Size([2, 2])\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# initializ a tensor from data\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(\"x_data: \\n\", x_data)\n",
    "print(\"Shape of tensor x_data: \", x_data.shape)\n",
    "print(f\"Device tensor is stored on: {x_data.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d54695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "np_array: \n",
      " [[1 2]\n",
      " [3 4]]\n",
      "Shape of np_array:  (2, 2)\n",
      "Datatype of np_array:  int64\n",
      "\n",
      "x_np: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape of tensor x_np:  torch.Size([2, 2])\n",
      "Datatype of tensor x_np:  torch.int64\n"
     ]
    }
   ],
   "source": [
    "# initlizing tensors from NumPy array\n",
    "np_array = np.array(data)\n",
    "print(\"\\nnp_array: \\n\", np_array)\n",
    "print(\"Shape of np_array: \", np_array.shape)\n",
    "print(\"Datatype of np_array: \", np_array.dtype)\n",
    "\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(\"\\nx_np: \\n\", x_np)\n",
    "print(\"Shape of tensor x_np: \", x_np.shape)\n",
    "print(\"Datatype of tensor x_np: \", x_np.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d624af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_zeros: \n",
      " tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "\n",
      "x_rand: \n",
      " tensor([[0.9151, 0.6225],\n",
      "        [0.1912, 0.4041]])\n",
      "Datatype of tensor x_rand:  torch.float32\n"
     ]
    }
   ],
   "source": [
    "# we can also create a tensor from another tensor\n",
    "x_zeros = torch.zeros_like(x_data) # retains the properties of x_data - shape, datatype\n",
    "print(\"\\nx_zeros: \\n\", x_zeros)\n",
    "\n",
    "# rand_like returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0, 1)\n",
    "# the input of rand_like is a tensor\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.rand_like.html\n",
    "x_rand = torch.rand_like(input=x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(\"\\nx_rand: \\n\", x_rand)\n",
    "print(\"Datatype of tensor x_rand: \", x_rand.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a683db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_random: \n",
      " tensor([[0.6013, 0.7835, 0.3469],\n",
      "        [0.8089, 0.8830, 0.7346]])\n",
      "\n",
      "ones_tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "zeros_tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Random Tensor of shape  (3, 4) : \n",
      " tensor([[0.7980, 0.9271, 0.8042, 0.4433],\n",
      "        [0.3317, 0.8314, 0.0136, 0.9851],\n",
      "        [0.8103, 0.1372, 0.4657, 0.2968]])\n"
     ]
    }
   ],
   "source": [
    "# we can create tensors using random or constant values\n",
    "# rand returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)\n",
    "# difference between rand and rand_like is that rand requires the shape of the tensor to be passed as argument\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.rand.html\n",
    "x_random = torch.rand(2,3) # random tensor of shape 2x3\n",
    "print(\"\\nx_random: \\n\", x_random)\n",
    "ones_tensor = torch.ones(2,3) # tensor of ones of shape 2x3\n",
    "print(\"\\nones_tensor: \\n\", ones_tensor)\n",
    "zeros_tensor = torch.zeros(2,3) # tensor of zeros of shape 2x3\n",
    "print(\"\\nzeros_tensor: \\n\", zeros_tensor)\n",
    "\n",
    "# shape can also be a tuple\n",
    "shape = (3,4)\n",
    "rand_tensor = torch.rand(size=shape)\n",
    "print(\"\\nRandom Tensor of shape \", shape, \": \\n\", rand_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce26ad77",
   "metadata": {},
   "source": [
    "### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c2caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# all tensor operations found here: https://docs.pytorch.org/docs/stable/torch.html\n",
    "torch_data = torch.tensor(data=([1,2], [3,4]))\n",
    "\n",
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = torch_data.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\") # Device tensor is stored on: cuda:0\n",
    "else:\n",
    "  print(f\"Device tensor is stored on: {torch_data.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4087ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_data: \n",
      " tensor([[0.6720, 0.5291, 0.3283, 0.4763],\n",
      "        [0.4882, 0.9211, 0.8340, 0.4412],\n",
      "        [0.3542, 0.8879, 0.4125, 0.6154]])\n",
      "\n",
      "tensor_slice: \n",
      " tensor([[0.5291, 0.3283],\n",
      "        [0.9211, 0.8340]])\n"
     ]
    }
   ],
   "source": [
    "# indexing and slicing\n",
    "x_data = torch.rand(3,4)\n",
    "print(\"\\nx_data: \\n\", x_data)\n",
    "\n",
    "# we take the first two rows and columns 1 and 2\n",
    "tensor_slice = x_data[0:2, 1:3]\n",
    "print(\"\\ntensor_slice: \\n\", tensor_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "111e4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_data after setting column 1 to zeros: \n",
      " tensor([[0.6720, 0.0000, 0.3283, 0.4763],\n",
      "        [0.4882, 0.0000, 0.8340, 0.4412],\n",
      "        [0.3542, 0.0000, 0.4125, 0.6154]])\n"
     ]
    }
   ],
   "source": [
    "# we can also change values using indexing\n",
    "x_data[:,1] = 0\n",
    "print(\"\\nx_data after setting column 1 to zeros: \\n\", x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1838a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t1 - concatenated along rows: \n",
      " tensor([[0.6720, 0.0000, 0.3283, 0.4763],\n",
      "        [0.4882, 0.0000, 0.8340, 0.4412],\n",
      "        [0.3542, 0.0000, 0.4125, 0.6154],\n",
      "        [0.6720, 0.0000, 0.3283, 0.4763],\n",
      "        [0.4882, 0.0000, 0.8340, 0.4412],\n",
      "        [0.3542, 0.0000, 0.4125, 0.6154]])\n",
      "\n",
      "t2 - concatenated along columns: \n",
      " tensor([[0.6720, 0.0000, 0.3283, 0.4763, 0.6720, 0.0000, 0.3283, 0.4763],\n",
      "        [0.4882, 0.0000, 0.8340, 0.4412, 0.4882, 0.0000, 0.8340, 0.4412],\n",
      "        [0.3542, 0.0000, 0.4125, 0.6154, 0.3542, 0.0000, 0.4125, 0.6154]])\n"
     ]
    }
   ],
   "source": [
    "# we can join tensors\n",
    "# dim=0 refers to rows\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.cat.html\n",
    "t1 = torch.cat([x_data, x_data], dim=0) # concatenate along rows\n",
    "print(\"\\nt1 - concatenated along rows: \\n\", t1)\n",
    "\n",
    "# dim=1 refers to columns\n",
    "t2 = torch.cat([x_data, x_data], dim=1) # concatenate along columns\n",
    "print(\"\\nt2 - concatenated along columns: \\n\", t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e03dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_data: \n",
      " tensor([[0.8882, 0.2182, 0.5246, 0.0376],\n",
      "        [0.4700, 0.8707, 0.7633, 0.5844],\n",
      "        [0.1757, 0.1779, 0.9806, 0.4966]])\n",
      "Shape of tensor x_data:  torch.Size([3, 4]) \n",
      "\n",
      "Shape of stacked tensor t3 with dim=0:  torch.Size([2, 3, 4])\n",
      "t3: \n",
      " tensor([[[0.8882, 0.2182, 0.5246, 0.0376],\n",
      "         [0.4700, 0.8707, 0.7633, 0.5844],\n",
      "         [0.1757, 0.1779, 0.9806, 0.4966]],\n",
      "\n",
      "        [[0.8882, 0.2182, 0.5246, 0.0376],\n",
      "         [0.4700, 0.8707, 0.7633, 0.5844],\n",
      "         [0.1757, 0.1779, 0.9806, 0.4966]]]) \n",
      "\n",
      "Shape of stacked tensor t4 with dim=1:  torch.Size([3, 2, 4])\n",
      "t4: \n",
      " tensor([[[0.8882, 0.2182, 0.5246, 0.0376],\n",
      "         [0.8882, 0.2182, 0.5246, 0.0376]],\n",
      "\n",
      "        [[0.4700, 0.8707, 0.7633, 0.5844],\n",
      "         [0.4700, 0.8707, 0.7633, 0.5844]],\n",
      "\n",
      "        [[0.1757, 0.1779, 0.9806, 0.4966],\n",
      "         [0.1757, 0.1779, 0.9806, 0.4966]]]) \n",
      "\n",
      "Shape of stacked tensor t5 with dim=2:  torch.Size([3, 4, 2])\n",
      "t5: \n",
      " tensor([[[0.8882, 0.8882],\n",
      "         [0.2182, 0.2182],\n",
      "         [0.5246, 0.5246],\n",
      "         [0.0376, 0.0376]],\n",
      "\n",
      "        [[0.4700, 0.4700],\n",
      "         [0.8707, 0.8707],\n",
      "         [0.7633, 0.7633],\n",
      "         [0.5844, 0.5844]],\n",
      "\n",
      "        [[0.1757, 0.1757],\n",
      "         [0.1779, 0.1779],\n",
      "         [0.9806, 0.9806],\n",
      "         [0.4966, 0.4966]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also use stack to join tensors\n",
    "# dim refers to the dimension along which the tensors will be stacked\n",
    "# dim (int, optional) – dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive). Default: 0\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.stack.html\n",
    "x_data = torch.rand(3,4)\n",
    "print(\"\\nx_data: \\n\", x_data)\n",
    "print(\"Shape of tensor x_data: \", x_data.shape, \"\\n\")\n",
    "\n",
    "t3 = torch.stack([x_data, x_data], dim=0) # stacks tensors along a new dimension\n",
    "print(\"Shape of stacked tensor t3 with dim=0: \", t3.shape)\n",
    "print(\"t3: \\n\", t3, \"\\n\")\n",
    "\n",
    "t4 = torch.stack([x_data, x_data], dim=1) # stacks tensors along a new dimension\n",
    "print(\"Shape of stacked tensor t4 with dim=1: \", t4.shape)\n",
    "print(\"t4: \\n\", t4, \"\\n\")\n",
    "\n",
    "t5 = torch.stack([x_data, x_data], dim=2) # stacks tensors along a new dimension\n",
    "print(\"Shape of stacked tensor t5 with dim=2: \", t5.shape)\n",
    "print(\"t5: \\n\", t5, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197e717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data: \n",
      " tensor([[9, 9, 4],\n",
      "        [1, 2, 0]])\n",
      "y_data: \n",
      " tensor([[5, 1, 1],\n",
      "        [6, 8, 7]])\n",
      "\n",
      "z_data (element-wise multiplication): \n",
      " tensor([[45,  9,  4],\n",
      "        [ 6, 16,  0]])\n",
      "\n",
      "z_data_alt (element-wise multiplication using torch.mul): \n",
      " tensor([[45,  9,  4],\n",
      "        [ 6, 16,  0]])\n"
     ]
    }
   ],
   "source": [
    "# multiplying tensors\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.randint.html\n",
    "x_data = torch.randint(low=0, high=10, size=(2, 3))\n",
    "y_data = torch.randint(low=0, high=10, size=(2, 3))\n",
    "print(\"x_data: \\n\", x_data)\n",
    "print(\"y_data: \\n\", y_data)\n",
    "\n",
    "# element-wise multiplication\n",
    "z_data = x_data * y_data\n",
    "print(\"\\nz_data (element-wise multiplication): \\n\", z_data)\n",
    "\n",
    "# alternate syntax\n",
    "z_data_alt = torch.mul(x_data, y_data)\n",
    "print(\"\\nz_data_alt (element-wise multiplication using torch.mul): \\n\", z_data_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca862488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_data: \n",
      " tensor([[2, 9, 4],\n",
      "        [8, 6, 8]])\n",
      "y_data: \n",
      " tensor([[8, 8],\n",
      "        [8, 9],\n",
      "        [7, 7]])\n",
      "\n",
      "z_data (matrix multiplication using torch.matmul): \n",
      " tensor([[116, 125],\n",
      "        [168, 174]])\n",
      "\n",
      "z_data_alt (matrix multiplication using torch.mm and transpose): \n",
      " tensor([[ 53, 108],\n",
      "        [ 54, 136]])\n",
      "\n",
      "z_data_alt (matrix multiplication using torch.mm and alternative transpose): \n",
      " tensor([[ 53, 108],\n",
      "        [ 54, 136]])\n",
      "\n",
      "z_data_at (matrix multiplication using @ operator): \n",
      " tensor([[ 53, 108],\n",
      "        [ 54, 136]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "x_data = torch.randint(low=0, high=10, size=(2, 3))\n",
    "y_data = torch.randint(low=0, high=10, size=(3, 2))\n",
    "print(\"\\nx_data: \\n\", x_data)\n",
    "print(\"y_data: \\n\", y_data)\n",
    "\n",
    "z_data = torch.matmul(x_data, y_data)\n",
    "print(\"\\nz_data (matrix multiplication using torch.matmul): \\n\", z_data)\n",
    "\n",
    "# using Transpose for matrix multiplication\n",
    "y_data = torch.randint(low=0, high=10, size=(2, 3))\n",
    "\n",
    "# using .t() method for transpose\n",
    "y_dataT = torch.t(y_data) \n",
    "z_data_alt = torch.matmul(x_data, y_dataT)\n",
    "print(\"\\nz_data_alt (matrix multiplication using torch.mm and transpose): \\n\", z_data_alt)\n",
    "\n",
    "# alternative for transpose using .T attribute\n",
    "z_data_alt = torch.matmul(x_data, y_data.T)\n",
    "print(\"\\nz_data_alt (matrix multiplication using torch.mm and alternative transpose): \\n\", z_data_alt)\n",
    "\n",
    "# alternately we can use the @ operator for matrix multiplication\n",
    "z_data_at = x_data @ y_dataT\n",
    "print(\"\\nz_data_at (matrix multiplication using @ operator): \\n\", z_data_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952c04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor: \n",
      " tensor([[0.4962, 0.0981, 0.4911, 0.7548],\n",
      "        [0.3078, 0.8141, 0.4804, 0.9459],\n",
      "        [0.2776, 0.7519, 0.4350, 0.5456]])\n",
      "Shape of tensor:  torch.Size([3, 4])\n",
      "Datatype of tensor:  torch.float32\n",
      "\n",
      "numpy_array converted from tensor: \n",
      " [[0.49616057 0.09806406 0.49114263 0.75475955]\n",
      " [0.3078339  0.8140762  0.48040283 0.94590724]\n",
      " [0.2775588  0.7518571  0.43497956 0.54560083]]\n",
      "Shape of numpy_array:  (3, 4)\n",
      "Datatype of numpy_array:  float32\n"
     ]
    }
   ],
   "source": [
    "# we can also convert tensor to numpy\n",
    "tensor = torch.rand(3,4)\n",
    "print(\"\\ntensor: \\n\", tensor)\n",
    "print(\"Shape of tensor: \", tensor.shape)\n",
    "print(\"Datatype of tensor: \", tensor.dtype)\n",
    "\n",
    "numpy_array = tensor.numpy()\n",
    "print(\"\\nnumpy_array converted from tensor: \\n\", numpy_array)\n",
    "print(\"Shape of numpy_array: \", numpy_array.shape)\n",
    "print(\"Datatype of numpy_array: \", numpy_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3f947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numpy_array: \n",
      " [[0.40100405 0.78795929 0.15280297 0.86562914]\n",
      " [0.86783988 0.63264958 0.21388667 0.73238266]\n",
      " [0.36621513 0.78463168 0.41143303 0.7874434 ]]\n",
      "Shape of numpy_array:  (3, 4)\n",
      "Datatype of numpy_array:  float64\n",
      "\n",
      "tensor_from_numpy: \n",
      " tensor([[0.4010, 0.7880, 0.1528, 0.8656],\n",
      "        [0.8678, 0.6326, 0.2139, 0.7324],\n",
      "        [0.3662, 0.7846, 0.4114, 0.7874]], dtype=torch.float64)\n",
      "Shape of tensor_from_numpy:  torch.Size([3, 4])\n",
      "Datatype of tensor_from_numpy:  torch.float64\n"
     ]
    }
   ],
   "source": [
    "# or change numpy array to tensor\n",
    "numpy_array = np.random.rand(3,4)\n",
    "print(\"\\nnumpy_array: \\n\", numpy_array)\n",
    "print(\"Shape of numpy_array: \", numpy_array.shape)\n",
    "print(\"Datatype of numpy_array: \", numpy_array.dtype)\n",
    "\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(\"\\ntensor_from_numpy: \\n\", tensor_from_numpy)\n",
    "print(\"Shape of tensor_from_numpy: \", tensor_from_numpy.shape)\n",
    "print(\"Datatype of tensor_from_numpy: \", tensor_from_numpy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor x: \n",
      " tensor([[0.4424, 0.2747, 0.9263, 0.8910],\n",
      "        [0.5453, 0.8486, 0.7580, 0.5367],\n",
      "        [0.2699, 0.9591, 0.8523, 0.2504]])\n",
      "Memory address of x before in-place operation:  0x74d1aeba3070\n",
      "\n",
      "Tensor x after in-place addition of 5: \n",
      " tensor([[5.4424, 5.2747, 5.9263, 5.8910],\n",
      "        [5.5453, 5.8486, 5.7580, 5.5367],\n",
      "        [5.2699, 5.9591, 5.8523, 5.2504]])\n",
      "Memory address of x after in-place operation:  0x74d1aeba3070\n"
     ]
    }
   ],
   "source": [
    "# we use in_place operations to save memory\n",
    "x = torch.rand(3,4)\n",
    "print(\"Original tensor x: \\n\", x)\n",
    "print(\"Memory address of x before in-place operation: \", hex(id(x)))\n",
    "x.add_(5)  # in-place addition\n",
    "print(\"\\nTensor x after in-place addition of 5: \\n\", x)\n",
    "print(\"Memory address of x after in-place operation: \", hex(id(x)))\n",
    "# other similar are copy_ or zero_ or fill_ or t_\n",
    "# \"In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f283d",
   "metadata": {},
   "source": [
    "## Difference between torch.mm, torch.matmul, torch.mul and torch.bmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea4f89",
   "metadata": {},
   "source": [
    "### torch.mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62e7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "b:\n",
      " tensor([[5, 6],\n",
      "        [7, 8]]) \n",
      "\n",
      "torch.mul(a, b):\n",
      " tensor([[ 5, 12],\n",
      "        [21, 32]]) \n",
      "\n",
      "Same as a * b:\n",
      " tensor([[ 5, 12],\n",
      "        [21, 32]])\n"
     ]
    }
   ],
   "source": [
    "# 1. torch.mul - Element-wise multiplication (works on any shape, no matrix multiplication)\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "print(\"a:\\n\", a, \"\\n\")\n",
    "print(\"b:\\n\", b, \"\\n\")\n",
    "print(\"torch.mul(a, b):\\n\", torch.mul(a, b), \"\\n\")\n",
    "print(\"Same as a * b:\\n\", a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a3580",
   "metadata": {},
   "source": [
    "### torch.mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da9929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (2x3):\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]) \n",
      "\n",
      "shape of x: torch.Size([2, 3]), dimensions: 2\n",
      "x is a 2D tensor\n",
      "\n",
      "y (3x2):\n",
      " tensor([[ 7,  8],\n",
      "        [ 9, 10],\n",
      "        [11, 12]]) \n",
      "\n",
      "shape of y: torch.Size([3, 2]), dimensions: 2\n",
      "y is a 2D tensor\n",
      "\n",
      "torch.mm(x, y) (2x2):\n",
      " tensor([[ 58,  64],\n",
      "        [139, 154]]) \n",
      "\n",
      "\n",
      "torch.mm only works with 2D tensors\n",
      "shape of vec: torch.Size([3]), dimensions: 1\n",
      "vec is a 1D tensor\n",
      "Error with 1D tensors: self must be a matrix\n"
     ]
    }
   ],
   "source": [
    "# torch.mm - Strict 2D matrix multiplication (only works with 2D tensors)\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 2x3\n",
    "print(\"x (2x3):\\n\", x, \"\\n\")\n",
    "print(f\"shape of x: {x.shape}, dimensions: {x.dim()}\")\n",
    "print(f\"x is a {x.dim()}D tensor\\n\")\n",
    "print(\"y (3x2):\\n\", y, \"\\n\")\n",
    "\n",
    "y = torch.tensor([[7, 8], [9, 10], [11, 12]])  # 3x2\n",
    "print(f\"shape of y: {y.shape}, dimensions: {y.dim()}\")\n",
    "print(f\"y is a {y.dim()}D tensor\\n\")\n",
    "print(\"torch.mm(x, y) (2x2):\\n\", torch.mm(x, y), \"\\n\")\n",
    "\n",
    "# torch.mm does NOT work with 1D or 3D tensors\n",
    "print(\"\\ntorch.mm only works with 2D tensors\")\n",
    "try:\n",
    "    vec = torch.tensor([1, 2, 3])\n",
    "    torch.mm(vec, vec)\n",
    "except RuntimeError as e:\n",
    "    print(f\"shape of vec: {vec.shape}, dimensions: {vec.dim()}\")\n",
    "    print(f\"vec is a {vec.dim()}D tensor\")\n",
    "    print(f\"Error with 1D tensors: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f84aa6",
   "metadata": {},
   "source": [
    "### torch.matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6accb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.matmul(x, y):\n",
      " tensor([[ 58,  64],\n",
      "        [139, 154]])\n",
      "\n",
      "Works with 1D vectors (dot product):\n",
      "v1: tensor([1, 2, 3])\n",
      "shape of v1: torch.Size([3]), dimensions: 1\n",
      "v2: tensor([4, 5, 6])\n",
      "shape of v2: torch.Size([3]), dimensions: 1\n",
      "torch.matmul(v1, v2): tensor(32) \n",
      "\n",
      "Batch: \n",
      " tensor([[[ 0.4326, -1.2180, -1.6467, -0.7321],\n",
      "         [ 0.7143, -0.1420, -1.1062,  0.3012]],\n",
      "\n",
      "        [[-2.0892,  1.0409, -0.3907, -0.8132],\n",
      "         [-0.0974, -1.1037,  0.8534, -0.1865]],\n",
      "\n",
      "        [[ 1.6558, -1.2848, -1.7281,  0.8026],\n",
      "         [-0.6132,  0.4482, -1.3220, -0.7041]]])\n",
      "batch shape: torch.Size([3, 2, 4]), dimensions: 3 \n",
      "\n",
      "mat: \n",
      " tensor([[ 1.2829,  0.7644,  1.5887,  1.3469, -0.4371],\n",
      "        [-0.7535,  2.4394,  0.9881, -0.9113, -1.1043],\n",
      "        [-0.7483,  1.8146, -1.3206,  0.3291,  1.3107],\n",
      "        [-0.6737, -0.1068,  0.1839, -0.7033,  1.5295]])\n",
      "mat shape: torch.Size([4, 5]), dimensions: 2\n",
      "\n",
      " torch.matmul(batch, mat) shape: torch.Size([3, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# torch.matmul - General matrix multiplication (supports broadcasting)\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 2x3\n",
    "y = torch.tensor([[7, 8], [9, 10], [11, 12]])  # 3x2\n",
    "print(\"torch.matmul(x, y):\\n\", torch.matmul(x, y))\n",
    "\n",
    "print(\"\\nWorks with 1D vectors (dot product):\")\n",
    "v1 = torch.tensor([1, 2, 3])\n",
    "v2 = torch.tensor([4, 5, 6])\n",
    "print(\"v1:\", v1)\n",
    "print(f\"shape of v1: {v1.shape}, dimensions: {v1.dim()}\")\n",
    "print(\"v2:\", v2)\n",
    "print(f\"shape of v2: {v2.shape}, dimensions: {v2.dim()}\")\n",
    "print(\"torch.matmul(v1, v2):\", torch.matmul(v1, v2), \"\\n\")\n",
    "\n",
    "# works with higher dimensions and broadcasts:\n",
    "# batch of matrices multiplied by single matrix\n",
    "batch = torch.randn(3, 2, 4)  # 3 matrices of size 2x4 \n",
    "mat = torch.randn(4, 5)  # single matrix of size 4x5\n",
    "\n",
    "# The single matrix (4x5) is broadcast and multiplied with each of the 3 matrices (2x4), giving 3 result matrices (2x5)\n",
    "result = torch.matmul(batch, mat)  # broadcasts to 3x2x5\n",
    "\n",
    "print(\"Batch: \\n\", batch)\n",
    "print(f\"batch shape: {batch.shape}, dimensions: {batch.dim()}\", \"\\n\")\n",
    "print(\"mat: \\n\", mat)\n",
    "print(f\"mat shape: {mat.shape}, dimensions: {mat.dim()}\")\n",
    "print(f\"\\n torch.matmul(batch, mat) shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482504c",
   "metadata": {},
   "source": [
    "### torch.bmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba5167af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1 shape: torch.Size([10, 3, 4])\n",
      "batch2 shape: torch.Size([10, 4, 5])\n",
      "torch.bmm(batch1, batch2) shape: torch.Size([10, 3, 5])\n",
      "\n",
      "batch_a shape: torch.Size([5, 2, 3]), batch_b shape: torch.Size([3, 4])\n",
      "torch.matmul can broadcast: torch.Size([5, 2, 4])\n",
      "torch.bmm cannot broadcast: batch2 must be a 3D tensor\n"
     ]
    }
   ],
   "source": [
    "# torch.bmm requires EXACTLY 3D tensors with same batch size\n",
    "batch1 = torch.randn(10, 3, 4) # 10 matrices of size 3x4\n",
    "batch2 = torch.randn(10, 4, 5) # 10 matrices of size 4x5\n",
    "result = torch.bmm(batch1, batch2)  # results in 10x3x5\n",
    "print(f\"batch1 shape: {batch1.shape}\")\n",
    "print(f\"batch2 shape: {batch2.shape}\")\n",
    "print(f\"torch.bmm(batch1, batch2) shape: {result.shape}\")\n",
    "\n",
    "# Example where matmul works but bmm doesn't (broadcasting)\n",
    "batch_a = torch.randn(5, 2, 3)  # 5 matrices of 2x3\n",
    "batch_b = torch.randn(3, 4)     # single matrix of 3x4\n",
    "\n",
    "print(f\"\\nbatch_a shape: {batch_a.shape}, batch_b shape: {batch_b.shape}\")\n",
    "print(\"torch.matmul can broadcast:\", torch.matmul(batch_a, batch_b).shape)\n",
    "try:\n",
    "    torch.bmm(batch_a, batch_b)\n",
    "except RuntimeError as e:\n",
    "    print(f\"torch.bmm cannot broadcast: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebaf563",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (2x3):\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "scalar: 10\n",
      "\n",
      "a + scalar (scalar is broadcast to match a's shape):\n",
      " tensor([[11, 12, 13],\n",
      "        [14, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting - Operating on tensors with different shapes\n",
    "# Broadcasting allows PyTorch to automatically expand tensors to compatible shapes\n",
    "\n",
    "# Scalar broadcasting\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "scalar = 10\n",
    "print(\"a (2x3):\\n\", a)\n",
    "print(\"scalar:\", scalar)\n",
    "print(\"\\na + scalar (scalar is broadcast to match a's shape):\\n\", a + scalar) #scalar 10 becomes [[10, 10, 10], [10, 10, 10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfda30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix (2x3):\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "vector (3,): tensor([10, 20, 30])\n",
      "\n",
      "matrix + vector:\n",
      " tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n"
     ]
    }
   ],
   "source": [
    "# 1D to 2D broadcasting\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 2x3\n",
    "vector = torch.tensor([10, 20, 30])  # 3\n",
    "print(\"matrix (2x3):\\n\", matrix)\n",
    "print(\"vector (3,):\", vector)\n",
    "print(\"\\nmatrix + vector:\\n\", matrix + vector)\n",
    "\n",
    "#  vector [10, 20, 30] is broadcast to:\n",
    "# [[10, 20, 30],\n",
    "#  [10, 20, 30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3443423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix (2x3):\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "col_vector (2x1):\n",
      " tensor([[10],\n",
      "        [20]])\n",
      "\n",
      "matrix + col_vector:\n",
      " tensor([[11, 12, 13],\n",
      "        [24, 25, 26]])\n"
     ]
    }
   ],
   "source": [
    "# Column vector broadcasting\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 2x3\n",
    "col_vector = torch.tensor([[10], [20]])  # 2x1\n",
    "print(\"matrix (2x3):\\n\", matrix)\n",
    "print(\"col_vector (2x1):\\n\", col_vector)\n",
    "print(\"\\nmatrix + col_vector:\\n\", matrix + col_vector)\n",
    "\n",
    "# col_vector [[10], [20]] is broadcast to:\n",
    "# [[10, 20, 30],\n",
    "#  [10, 20, 30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98b9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a shape: torch.Size([3, 1, 4]) (3, 1, 4)\n",
      "b shape: torch.Size([1, 5, 4]) (1, 5, 4)\n",
      "Result shape: torch.Size([3, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# \"Two tensors are broadcastable if:\"\n",
    "# \"1. Each tensor has at least one dimension, AND\"\n",
    "# \"2. When iterating over dimensions from right to left:\n",
    "#     - Dimensions are equal, OR\"\n",
    "#     - One of them is 1, OR\"\n",
    "#     - One of them doesn't exist\"\n",
    "\n",
    "# Example of compatible shapes\n",
    "a = torch.randn(3, 1, 4)\n",
    "b = torch.randn(1, 5, 4)\n",
    "c = a + b\n",
    "print(f\"\\na shape: {a.shape} (3, 1, 4)\")\n",
    "print(f\"b shape: {b.shape} (1, 5, 4)\")\n",
    "print(f\"Result shape: {c.shape}\")\n",
    "\n",
    "#   a: (3, 1, 4)\n",
    "#   b: (1, 5, 4)\n",
    "#       ↑  ↑  ↑\n",
    "#     dim0 dim1 dim2\n",
    "\n",
    "# Dim 2 (rightmost): a[4] vs b[4] → 4 == 4 ✓ Result: 4\n",
    "# Dim 1 (middle): a[1] vs b[5] → 1 can broadcast to 5 ✓ Result: 5\n",
    "#     - When one dimension is 1, it stretches/repeats to match the other\n",
    "# Dim 0 (leftmost): a[3] vs b[1] → 1 can broadcast to 3 ✓ Result: 3\n",
    "#     - Again, 1 broadcasts to match 3\n",
    "\n",
    "# Final result: (3, 5, 4)\n",
    "\n",
    "# What actually happens:\n",
    "# - Tensor a (3, 1, 4): The middle dimension [1] is repeated 5 times\n",
    "# - Tensor b (1, 5, 4): The first dimension [1] is repeated 3 times\n",
    "\n",
    "# So effectively:\n",
    "# - a becomes (3, 5, 4) by copying its single column 5 times\n",
    "# - b becomes (3, 5, 4) by copying its single matrix 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "071f34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([3, 4]) (3, 4)\n",
      "y shape: torch.Size([3, 5]) (3, 5)\n",
      "\n",
      "Dimension by dimension:\n",
      "  Dim 1: 4 != 5 and neither is 1 ✗\n",
      "\n",
      "Error: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Incompatible shapes\n",
    "x = torch.randn(3, 4)\n",
    "y = torch.randn(3, 5)\n",
    "print(f\"x shape: {x.shape} (3, 4)\")\n",
    "print(f\"y shape: {y.shape} (3, 5)\")\n",
    "print(\"\\nDimension by dimension:\")\n",
    "print(\"  Dim 1: 4 != 5 and neither is 1 ✗\")\n",
    "try:\n",
    "    z = x + y\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "\n",
    "\n",
    "#   x: (3, 4)\n",
    "#   y: (3, 5)\n",
    "#       ↑  ↑\n",
    "#      dim0 dim1\n",
    "\n",
    "# 1. Dim 1 (rightmost): x[4] vs y[5] → 4 ≠ 5 and neither is 1 ✗ FAIL\n",
    "\n",
    "# Broadcasting stops here!\n",
    "\n",
    "# Why it fails:\n",
    "# - For broadcasting to work, dimensions must be:\n",
    "# - Equal (e.g., 4 == 4), OR\n",
    "# - One of them is 1 (e.g., 1 can broadcast to any size), OR\n",
    "# - One doesn't exist (e.g., (3,) can broadcast with (3, 4))\n",
    "\n",
    "# Since 4 ≠ 5 and neither dimension is 1, PyTorch cannot automatically expand either tensor to make them compatible.\n",
    "\n",
    "# If we wanted them to work:\n",
    "# - Change y to (3, 4) → dimensions match\n",
    "# - Change y to (3, 1) → the 1 can broadcast to 4\n",
    "# - Change x to (3, 1) → the 1 can broadcast to 5\n",
    "\n",
    "# But with 4 and 5, there's no way to broadcast - PyTorch won't guess which one you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c9e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf54f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cookbook-for-noobs (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
